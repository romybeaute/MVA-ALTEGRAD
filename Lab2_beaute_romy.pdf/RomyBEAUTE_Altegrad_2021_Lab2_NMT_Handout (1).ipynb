{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RomyBEAUTE_Altegrad_2021_Lab2_NMT_Handout.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCJvlnvsKALE"
      },
      "source": [
        "<center><h2>ALTeGraD 2021<br>Lab Session 2: NMT</h2><h3> Neural Machine Translation</h3> 16 / 11 / 2021<br> M. Kamal Eddine, H. Abdine</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4zDJsvrrJJ-"
      },
      "source": [
        "## Notes about RNN\n",
        "- CNNs are good at dealing with grids\n",
        "- RNNs were specifically developed to be used with sequences\n",
        "- a RNN can be viewed as a chain of **simple neural** layers that **share the same parameters**.\n",
        "\n",
        "###Implementation of RNN :\n",
        "From a high level:\n",
        "- input :  RNN is fed an ordered list of **input vectors {x1 , ..., xT}** (i.e. source sentences) + an **initial hidden state h0** initialized to all zeros \n",
        "- output : returns an ordered list of **hidden states {h1 , ..., hT}** + an ordered list of **output vectors {y1 , ..., yT}** (i.e. target sentences)\n",
        "\n",
        "### Hidden states :\n",
        "- may serve as **input to the RNN units above** (stacked architecture)\n",
        "- or can be directly used as they are (e.g., by the attention mechanism). \\\\\n",
        "==> The hidden states correspond more or less to the “short-term” memory of the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DB6pvLvlKbtD"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils import data\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from tqdm import tqdm\n",
        "from nltk import word_tokenize\n",
        "from itertools import chain\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqIFlSfYTwk8"
      },
      "source": [
        "## Define the Encoder / Task 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kc8cQTFkKmif"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    '''\n",
        "    to be passed the entire source sequence at once\n",
        "    we use padding_idx in nn.Embedding so that the padding vector does not take gradient (always zero)\n",
        "    https://pytorch.org/docs/stable/nn.html#gru\n",
        "    '''\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, padding_idx):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx)\n",
        "        self.rnn = nn.GRU(embedding_dim, hidden_dim)\n",
        "    \n",
        "    def forward(self, input):\n",
        "      x = self.embedding(input) # (transform input into embeddings and pass embeddings to RNN)\n",
        "      hs = self.rnn(x) # return a tensor of shape (seq, batch, feat)\n",
        "      return hs[0]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bn9iO9wNT2p7"
      },
      "source": [
        "## Define the Attention layer / Task 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwUAUDL4KmoM"
      },
      "source": [
        "class seq2seqAtt(nn.Module):\n",
        "    '''\n",
        "    concat global attention a la Luong et al. 2015 (subsection 3.1)\n",
        "    https://arxiv.org/pdf/1508.04025.pdf\n",
        "    '''\n",
        "    def __init__(self, hidden_dim, hidden_dim_s, hidden_dim_t):\n",
        "        super(seq2seqAtt, self).__init__()\n",
        "        self.ff_concat = nn.Linear(hidden_dim_s+hidden_dim_t, hidden_dim)\n",
        "        self.ff_score = nn.Linear(hidden_dim, 1, bias=False) # just a dot product here\n",
        "    \n",
        "    def forward(self, target_h, source_hs):\n",
        "        target_h_rep = target_h.repeat(source_hs.size(0), 1, 1) # (1, batch, feat) -> (seq, batch, feat)\n",
        "        # fill the gaps #\n",
        "        # implement the score computation part of the concat formulation (see section 3.1. of Luong 2015)\n",
        "        concat_output = self.ff_concat(torch.concat((target_h_rep, source_hs), dim = 2))\n",
        "        scores = self.ff_score(torch.tanh(concat_output)) # should be of shape (seq, batch, 1)\n",
        "        scores = scores.squeeze(dim=2) # (seq, batch, 1) -> (seq, batch). dim = 2 because we don't want to squeeze the batch dim if batch size = 1\n",
        "        norm_scores = torch.softmax(scores, 0)\n",
        "        source_hs_p = source_hs.permute((2, 0, 1)) # (seq, batch, feat) -> (feat, seq, batch)\n",
        "        weighted_source_hs = (norm_scores * source_hs_p) # (seq, batch) * (feat, seq, batch) (* checks from right to left that the dimensions match)\n",
        "        ct = torch.sum(weighted_source_hs.permute((1, 2, 0)), 0, keepdim=True) # (feat, seq, batch) -> (seq, batch, feat) -> (1, batch, feat); keepdim otherwise sum squeezes \n",
        "        return ct"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNnGEa5cT9ka"
      },
      "source": [
        "## Define the Decoder layer / Task 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7tLaq4PK90q"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    '''to be used one timestep at a time\n",
        "       see https://pytorch.org/docs/stable/nn.html#gru'''\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, padding_idx):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx)\n",
        "        self.rnn = nn.GRU(embedding_dim, hidden_dim)\n",
        "        self.ff_concat = nn.Linear(2*hidden_dim, hidden_dim)\n",
        "        self.predict = nn.Linear(hidden_dim, vocab_size)\n",
        "    \n",
        "    def forward(self, input, source_context, h):\n",
        "        # fill the gaps #\n",
        "        # transform input into embeddings, pass embeddings to RNN, concatenate with source_context and apply tanh, and make the prediction\n",
        "        embedding = self.embedding(input) #transform input into embeddings\n",
        "        hs0, h = self.rnn(embedding, h) #pass embeddings to RNN\n",
        "        tilde_h = torch.tanh(self.ff_concat(torch.concat((source_context, h), dim = 2))) #concatenate with source_context and apply tanh\n",
        "        prediction = self.predict(tilde_h) #make the prediction\n",
        "        #print(prediction.shape(),tilde_h.shape()) # prediction should be of shape (1, batch, vocab), h and tilde_h of shape (1, batch, feat)\n",
        "        return prediction, h"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUT6D3JETX8H"
      },
      "source": [
        "# Define the full seq2seq model / Task 4:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYX0K3dNK-c9"
      },
      "source": [
        "class seq2seqModel(nn.Module):\n",
        "    '''the full seq2seq model'''\n",
        "    ARGS = ['vocab_s','source_language','vocab_t_inv','embedding_dim_s','embedding_dim_t',\n",
        "     'hidden_dim_s','hidden_dim_t','hidden_dim_att','do_att','padding_token',\n",
        "     'oov_token','sos_token','eos_token','max_size']\n",
        "    def __init__(self, vocab_s, source_language, vocab_t_inv, embedding_dim_s, embedding_dim_t, \n",
        "                 hidden_dim_s, hidden_dim_t, hidden_dim_att, do_att, padding_token,\n",
        "                 oov_token, sos_token, eos_token, max_size):\n",
        "        super(seq2seqModel, self).__init__()\n",
        "        self.vocab_s = vocab_s\n",
        "        self.source_language = source_language\n",
        "        self.vocab_t_inv = vocab_t_inv\n",
        "        self.embedding_dim_s = embedding_dim_s\n",
        "        self.embedding_dim_t = embedding_dim_t\n",
        "        self.hidden_dim_s = hidden_dim_s\n",
        "        self.hidden_dim_t = hidden_dim_t\n",
        "        self.hidden_dim_att = hidden_dim_att\n",
        "        self.do_att = do_att # should attention be used?\n",
        "        self.padding_token = padding_token\n",
        "        self.oov_token = oov_token\n",
        "        self.sos_token = sos_token\n",
        "        self.eos_token = eos_token\n",
        "        self.max_size = max_size\n",
        "        \n",
        "        self.max_source_idx = max(list(vocab_s.values()))\n",
        "        print('max source index',self.max_source_idx)\n",
        "        print('source vocab size',len(vocab_s))\n",
        "        \n",
        "        self.max_target_idx = max([int(elt) for elt in list(vocab_t_inv.keys())])\n",
        "        print('max target index',self.max_target_idx)\n",
        "        print('target vocab size',len(vocab_t_inv))\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.encoder = Encoder(self.max_source_idx+1, self.embedding_dim_s, self.hidden_dim_s, self.padding_token).to(self.device)\n",
        "        self.decoder = Decoder(self.max_target_idx+1, self.embedding_dim_t, self.hidden_dim_t, self.padding_token).to(self.device)\n",
        "        \n",
        "        if self.do_att:\n",
        "            self.att_mech = seq2seqAtt(self.hidden_dim_att, self.hidden_dim_s, self.hidden_dim_t).to(self.device)\n",
        "    \n",
        "    def my_pad(self, my_list):\n",
        "        '''my_list is a list of tuples of the form [(tensor_s_1, tensor_t_1), ..., (tensor_s_batch, tensor_t_batch)]\n",
        "        the <eos> token is appended to each sequence before padding\n",
        "        https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pad_sequence'''\n",
        "        batch_source = pad_sequence([torch.cat((elt[0], torch.LongTensor([self.eos_token]))) for elt in my_list], batch_first=True, padding_value=self.padding_token)\n",
        "        batch_target = pad_sequence([torch.cat((elt[1], torch.LongTensor([self.eos_token]))) for elt in my_list], batch_first=True, padding_value=self.padding_token)\n",
        "        return batch_source, batch_target\n",
        "    \n",
        "    def forward(self, input, max_size, is_prod):\n",
        "        if is_prod: \n",
        "            input = input.unsqueeze(1) # (seq) -> (seq, 1) 1D input <=> we receive just one sentence as input (predict/production mode)\n",
        "        current_batch_size = input.size(1)\n",
        "        # fill the gap #\n",
        "        # use the encoder\n",
        "        source_hs = self.encoder(input)\n",
        "        # = = = decoder part (one timestep at a time)  = = =\n",
        "        target_h = torch.zeros(size=(1, current_batch_size, self.hidden_dim_t)).to(self.device) # init (1, batch, feat)\n",
        "        \n",
        "        # fill the gap #\n",
        "        # (initialize target_input with the proper token)\n",
        "        target_input = torch.LongTensor([self.sos_token]).repeat(current_batch_size).unsqueeze(0).to(self.device) # init (1, batch)\n",
        "        pos = 0\n",
        "        eos_counter = 0\n",
        "        logits = []\n",
        "        scores = []\n",
        "        \n",
        "        while True:\n",
        "            if self.do_att:\n",
        "                source_context = self.att_mech(target_h, source_hs) # (1, batch, feat)\n",
        "            else:\n",
        "                source_context = source_hs[-1, :, :].unsqueeze(0) # (1, batch, feat) last hidden state of encoder\n",
        "            # fill the gap #\n",
        "            # use the decoder\n",
        "            prediction, target_h = self.decoder(target_input, source_context, target_h)\n",
        "            logits.append(prediction) # (1, batch, vocab)\n",
        "            # fill the gap #\n",
        "            # get the next input to pass the decoder\n",
        "            _, target_input = torch.max(prediction,2)\n",
        "            eos_counter += torch.sum(target_input==self.eos_token).item()\n",
        "            pos += 1\n",
        "            if pos >= max_size or (eos_counter == current_batch_size and is_prod):\n",
        "                break\n",
        "        to_return = torch.cat(logits, 0) # logits is a list of tensors -> (seq, batch, vocab)\n",
        "        \n",
        "        if is_prod:\n",
        "            to_return = to_return.squeeze(dim=1) # (seq, vocab)\n",
        "        \n",
        "        return to_return\n",
        "    \n",
        "    def fit(self, trainingDataset, testDataset, lr, batch_size, n_epochs, patience):\n",
        "        parameters = [p for p in self.parameters() if p.requires_grad]\n",
        "        optimizer = optim.Adam(parameters, lr=lr)\n",
        "        criterion = torch.nn.CrossEntropyLoss(ignore_index=self.padding_token) # the softmax is inside the loss!\n",
        "        # https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n",
        "        # we pass a collate function to perform padding on the fly, within each batch\n",
        "        # this is better than truncation/padding at the dataset level\n",
        "        train_loader = data.DataLoader(trainingDataset, batch_size=batch_size, \n",
        "                                       shuffle=True, collate_fn=self.my_pad) # returns (batch, seq)\n",
        "        test_loader = data.DataLoader(testDataset, batch_size=512,\n",
        "                                      collate_fn=self.my_pad)\n",
        "        tdqm_dict_keys = ['loss', 'test loss']\n",
        "        tdqm_dict = dict(zip(tdqm_dict_keys, [0.0, 0.0]))\n",
        "        patience_counter = 1\n",
        "        patience_loss = 99999\n",
        "        \n",
        "        for epoch in range(n_epochs): \n",
        "            with tqdm(total=len(train_loader), unit_scale=True, postfix={'loss':0.0, 'test loss':0.0},\n",
        "                      desc=\"Epoch : %i/%i\" % (epoch, n_epochs-1), ncols=100) as pbar:\n",
        "                for loader_idx, loader in enumerate([train_loader, test_loader]):\n",
        "                    total_loss = 0\n",
        "                    # set model mode (https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "                    if loader_idx == 0:\n",
        "                        self.train()\n",
        "                    else:\n",
        "                        self.eval()\n",
        "                    for i, (batch_source, batch_target) in enumerate(loader):\n",
        "                        batch_source = batch_source.transpose(1, 0).to(self.device) # RNN needs (seq, batch, feat) but loader returns (batch, seq)                        \n",
        "                        batch_target = batch_target.transpose(1, 0).to(self.device) # (seq, batch)\n",
        "                        \n",
        "                        # are we using the model in production\n",
        "                        is_prod = len(batch_source.shape)==1 # if False, 2D input (seq, batch), i.e., train or test\n",
        "                        if is_prod:\n",
        "                            max_size = self.max_size\n",
        "                            self.eval()\n",
        "                        else:\n",
        "                            max_size = batch_target.size(0) # no need to continue generating after we've exceeded the length of the longest ground truth sequence\n",
        "                        \n",
        "                        unnormalized_logits = self.forward(batch_source, max_size, is_prod)\n",
        "                        sentence_loss = criterion(unnormalized_logits.flatten(end_dim=1), batch_target.flatten())\n",
        "                        total_loss += sentence_loss.item()                        \n",
        "                        tdqm_dict[tdqm_dict_keys[loader_idx]] = total_loss/(i+1)                       \n",
        "                        pbar.set_postfix(tdqm_dict)                     \n",
        "                        if loader_idx == 0:\n",
        "                            optimizer.zero_grad() # flush gradient attributes\n",
        "                            sentence_loss.backward() # compute gradients\n",
        "                            optimizer.step() # update\n",
        "                            pbar.update(1)\n",
        "            \n",
        "            if total_loss > patience_loss:\n",
        "                patience_counter += 1\n",
        "            else:\n",
        "                patience_loss = total_loss\n",
        "                patience_counter = 1 # reset\n",
        "            \n",
        "            if patience_counter > patience:\n",
        "                break\n",
        "    \n",
        "    def sourceNl_to_ints(self, source_nl):\n",
        "        '''converts natural language source sentence into source integers'''\n",
        "        source_nl_clean = source_nl.lower().replace(\"'\",' ').replace('-',' ')\n",
        "        source_nl_clean_tok = word_tokenize(source_nl_clean, self.source_language)\n",
        "        source_ints = [int(self.vocab_s[elt]) if elt in self.vocab_s else \\\n",
        "                       self.oov_token for elt in source_nl_clean_tok]\n",
        "        \n",
        "        source_ints = torch.LongTensor(source_ints).to(self.device)\n",
        "        return source_ints \n",
        "    \n",
        "    def targetInts_to_nl(self, target_ints):\n",
        "        '''converts integer target sentence into target natural language'''\n",
        "        return ['<PAD>' if elt==self.padding_token else '<OOV>' if elt==self.oov_token \\\n",
        "                else '<EOS>' if elt==self.eos_token else '<SOS>' if elt==self.sos_token\\\n",
        "                else self.vocab_t_inv[elt] for elt in target_ints]\n",
        "    \n",
        "    def predict(self, source_nl):\n",
        "        source_ints = self.sourceNl_to_ints(source_nl)\n",
        "        logits = self.forward(source_ints, self.max_size, True) # (seq) -> (<=max_size, vocab)\n",
        "        target_ints = logits.argmax(-1).squeeze() # (<=max_size, 1) -> (<=max_size)\n",
        "        target_nl = self.targetInts_to_nl(target_ints.tolist())\n",
        "        return ' '.join(target_nl)\n",
        "        \n",
        "    def save(self, path_to_file):\n",
        "        attrs = {attr:getattr(self,attr) for attr in self.ARGS}\n",
        "        attrs['state_dict'] = self.state_dict()\n",
        "        torch.save(attrs, path_to_file)\n",
        "    \n",
        "    @classmethod # a class method does not see the inside of the class (a static method does not take self as first argument)\n",
        "    def load(cls, path_to_file):\n",
        "        attrs = torch.load(path_to_file, map_location=lambda storage, loc: storage) # allows loading on CPU a model trained on GPU, see https://discuss.pytorch.org/t/on-a-cpu-device-how-to-load-checkpoint-saved-on-gpu-device/349/6\n",
        "        state_dict = attrs.pop('state_dict')\n",
        "        new = cls(**attrs) # * list and ** names (dict) see args and kwargs\n",
        "        new.load_state_dict(state_dict)\n",
        "        return new        "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5RprtnBK-ia"
      },
      "source": [
        "import sys\n",
        "import json\n",
        "\n",
        "import torch\n",
        "from torch.utils import data"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgkVw6lVUIT3"
      },
      "source": [
        "## Prepare the Data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "datl5SFtJ9Br",
        "outputId": "04be3f00-2331-4c37-bd9a-2cccfc55ceee"
      },
      "source": [
        "!wget -c \"https://onedrive.live.com/download?cid=AE69638675180117&resid=AE69638675180117%2199291&authkey=AMIEuRcvDQWgoZo\" -O \"data.zip\"\n",
        "!wget -c \"https://onedrive.live.com/download?cid=AE69638675180117&resid=AE69638675180117%2199292&authkey=ANLtZTfpmk6tcE0\" -O \"pretrained_moodle.pt\"\n",
        "!unzip data.zip\n",
        "\n",
        "path_to_data = './'\n",
        "path_to_save_models = './'"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-23 20:15:56--  https://onedrive.live.com/download?cid=AE69638675180117&resid=AE69638675180117%2199291&authkey=AMIEuRcvDQWgoZo\n",
            "Resolving onedrive.live.com (onedrive.live.com)... 13.107.42.13\n",
            "Connecting to onedrive.live.com (onedrive.live.com)|13.107.42.13|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://vgtdqw.am.files.1drv.com/y4mUOAcpR-MQqNS9RTxh1-3TC_q_ZEegrCW4BMmE-SVUhTGljFiwRMgZqqONjmE3eascs7EoYXsINmHwAxrgZBedMUUFdRYkP973NkU-gWaaqivKMlqZwziIRjL6cOL65Lx2CKDogY_mSSV71lTgs8mMjUSc8CL9a73FJZkSYSQi9wIq_QXpxIgfRQSTrDcD9jYU4PJMZR_d0urubQ2FZv8aQ/data.zip?download&psid=1 [following]\n",
            "--2021-11-23 20:15:58--  https://vgtdqw.am.files.1drv.com/y4mUOAcpR-MQqNS9RTxh1-3TC_q_ZEegrCW4BMmE-SVUhTGljFiwRMgZqqONjmE3eascs7EoYXsINmHwAxrgZBedMUUFdRYkP973NkU-gWaaqivKMlqZwziIRjL6cOL65Lx2CKDogY_mSSV71lTgs8mMjUSc8CL9a73FJZkSYSQi9wIq_QXpxIgfRQSTrDcD9jYU4PJMZR_d0urubQ2FZv8aQ/data.zip?download&psid=1\n",
            "Resolving vgtdqw.am.files.1drv.com (vgtdqw.am.files.1drv.com)... 13.107.42.12\n",
            "Connecting to vgtdqw.am.files.1drv.com (vgtdqw.am.files.1drv.com)|13.107.42.12|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8994805 (8.6M) [application/zip]\n",
            "Saving to: ‘data.zip’\n",
            "\n",
            "data.zip            100%[===================>]   8.58M  4.37MB/s    in 2.0s    \n",
            "\n",
            "2021-11-23 20:16:01 (4.37 MB/s) - ‘data.zip’ saved [8994805/8994805]\n",
            "\n",
            "--2021-11-23 20:16:01--  https://onedrive.live.com/download?cid=AE69638675180117&resid=AE69638675180117%2199292&authkey=ANLtZTfpmk6tcE0\n",
            "Resolving onedrive.live.com (onedrive.live.com)... 13.107.42.13\n",
            "Connecting to onedrive.live.com (onedrive.live.com)|13.107.42.13|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://vgtcqw.am.files.1drv.com/y4malBaD4UOhZRR5cJsAa6z-v-gmyLYZSs7NbighXaXKB6BQUJjwmrPAkI5SwAipa1WHBkfmq_aqtB_no3gtjswRvZe8viKSLEdkrHl6d7bRk74pj6NpazUloJzN4yGCy0ZfVmE-nzosegjrhJHfHuXQxG0Z6XMbf9QeYdiJKtXfA21UfjmgPaYtnKsSROaxBP_rG-7hTZFqwUaDF5HiuIRYA/pretrained_moodle.pt?download&psid=1 [following]\n",
            "--2021-11-23 20:16:02--  https://vgtcqw.am.files.1drv.com/y4malBaD4UOhZRR5cJsAa6z-v-gmyLYZSs7NbighXaXKB6BQUJjwmrPAkI5SwAipa1WHBkfmq_aqtB_no3gtjswRvZe8viKSLEdkrHl6d7bRk74pj6NpazUloJzN4yGCy0ZfVmE-nzosegjrhJHfHuXQxG0Z6XMbf9QeYdiJKtXfA21UfjmgPaYtnKsSROaxBP_rG-7hTZFqwUaDF5HiuIRYA/pretrained_moodle.pt?download&psid=1\n",
            "Resolving vgtcqw.am.files.1drv.com (vgtcqw.am.files.1drv.com)... 13.107.42.12\n",
            "Connecting to vgtcqw.am.files.1drv.com (vgtcqw.am.files.1drv.com)|13.107.42.12|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3284775 (3.1M) [application/octet-stream]\n",
            "Saving to: ‘pretrained_moodle.pt’\n",
            "\n",
            "pretrained_moodle.p 100%[===================>]   3.13M  2.31MB/s    in 1.4s    \n",
            "\n",
            "2021-11-23 20:16:04 (2.31 MB/s) - ‘pretrained_moodle.pt’ saved [3284775/3284775]\n",
            "\n",
            "Archive:  data.zip\n",
            " extracting: pairs_test_ints.txt     \n",
            " extracting: pairs_train_ints.txt    \n",
            " extracting: README.txt              \n",
            " extracting: vocab_source.json       \n",
            " extracting: vocab_target.json       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZCiFl61LPQj"
      },
      "source": [
        "class Dataset(data.Dataset):\n",
        "  def __init__(self, pairs):\n",
        "        self.pairs = pairs\n",
        "\n",
        "  def __len__(self):\n",
        "        return len(self.pairs) # total nb of observations\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "        source, target = self.pairs[idx] # one observation\n",
        "        return torch.LongTensor(source), torch.LongTensor(target)\n",
        "\n",
        "def load_pairs(train_or_test):\n",
        "    with open(path_to_data + 'pairs_' + train_or_test + '_ints.txt', 'r', encoding='utf-8') as file:\n",
        "        pairs_tmp = file.read().splitlines()\n",
        "    pairs_tmp = [elt.split('\\t') for elt in pairs_tmp]\n",
        "    pairs_tmp = [[[int(eltt) for eltt in elt[0].split()],[int(eltt) for eltt in \\\n",
        "                  elt[1].split()]] for elt in pairs_tmp]\n",
        "    return pairs_tmp"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCsAk4ILTkEc"
      },
      "source": [
        "## Training / Task 5:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSZ-cvSuLQVt",
        "outputId": "9d22fa5e-2d9f-4c98-9602-f792e3e802dd"
      },
      "source": [
        "do_att = True # should always be set to True\n",
        "is_prod = False # production mode or not\n",
        "\n",
        "if not is_prod:\n",
        "        \n",
        "    pairs_train = load_pairs('train')\n",
        "    pairs_test = load_pairs('test')\n",
        "    \n",
        "    with open(path_to_data + 'vocab_source.json','r') as file:\n",
        "        vocab_source = json.load(file) # word -> index\n",
        "    \n",
        "    with open(path_to_data + 'vocab_target.json','r') as file:\n",
        "        vocab_target = json.load(file) # word -> index\n",
        "    \n",
        "    vocab_target_inv = {v:k for k,v in vocab_target.items()} # index -> word\n",
        "    \n",
        "    print('data loaded')\n",
        "        \n",
        "    training_set = Dataset(pairs_train)\n",
        "    test_set = Dataset(pairs_test)\n",
        "    \n",
        "    print('data prepared')\n",
        "    \n",
        "    print('= = = attention-based model?:',str(do_att),'= = =')\n",
        "    \n",
        "    model = seq2seqModel(vocab_s=vocab_source,\n",
        "                         source_language='english',\n",
        "                         vocab_t_inv=vocab_target_inv,\n",
        "                         embedding_dim_s=40,\n",
        "                         embedding_dim_t=40,\n",
        "                         hidden_dim_s=30,\n",
        "                         hidden_dim_t=30,\n",
        "                         hidden_dim_att=20,\n",
        "                         do_att=do_att,\n",
        "                         padding_token=0,\n",
        "                         oov_token=1,\n",
        "                         sos_token=2,\n",
        "                         eos_token=3,\n",
        "                         max_size=30) # max size of generated sentence in prediction mode\n",
        "    \n",
        "    model.fit(training_set,test_set,lr=0.001,batch_size=64,n_epochs=20,patience=2)\n",
        "    model.save(path_to_save_models + 'my_model.pt')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data loaded\n",
            "data prepared\n",
            "= = = attention-based model?: True = = =\n",
            "max source index 5281\n",
            "source vocab size 5278\n",
            "max target index 7459\n",
            "target vocab size 7456\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch : 0/19: 100%|██████████████████| 2.13k/2.13k [07:14<00:00, 4.92it/s, loss=5.2, test loss=4.71]\n",
            "Epoch : 1/19: 100%|█████████████████| 2.13k/2.13k [07:10<00:00, 4.96it/s, loss=4.54, test loss=4.38]\n",
            "Epoch : 2/19: 100%|██████████████████| 2.13k/2.13k [07:13<00:00, 4.93it/s, loss=4.29, test loss=4.2]\n",
            "Epoch : 3/19: 100%|█████████████████| 2.13k/2.13k [07:08<00:00, 4.98it/s, loss=4.11, test loss=4.05]\n",
            "Epoch : 4/19: 100%|█████████████████| 2.13k/2.13k [07:16<00:00, 4.89it/s, loss=3.97, test loss=3.95]\n",
            "Epoch : 5/19: 100%|█████████████████| 2.13k/2.13k [07:16<00:00, 4.89it/s, loss=3.78, test loss=3.68]\n",
            "Epoch : 6/19: 100%|█████████████████| 2.13k/2.13k [07:13<00:00, 4.93it/s, loss=3.53, test loss=3.48]\n",
            "Epoch : 7/19: 100%|█████████████████| 2.13k/2.13k [07:15<00:00, 4.90it/s, loss=3.36, test loss=3.34]\n",
            "Epoch : 8/19: 100%|█████████████████| 2.13k/2.13k [07:12<00:00, 4.94it/s, loss=3.23, test loss=3.23]\n",
            "Epoch : 9/19: 100%|█████████████████| 2.13k/2.13k [07:16<00:00, 4.89it/s, loss=3.13, test loss=3.15]\n",
            "Epoch : 10/19: 100%|████████████████| 2.13k/2.13k [07:11<00:00, 4.94it/s, loss=3.04, test loss=3.08]\n",
            "Epoch : 11/19: 100%|████████████████| 2.13k/2.13k [07:11<00:00, 4.94it/s, loss=2.97, test loss=3.02]\n",
            "Epoch : 12/19: 100%|████████████████| 2.13k/2.13k [07:08<00:00, 4.98it/s, loss=2.91, test loss=2.97]\n",
            "Epoch : 13/19: 100%|████████████████| 2.13k/2.13k [07:14<00:00, 4.91it/s, loss=2.85, test loss=2.92]\n",
            "Epoch : 14/19: 100%|████████████████| 2.13k/2.13k [07:16<00:00, 4.89it/s, loss=2.81, test loss=2.89]\n",
            "Epoch : 15/19: 100%|████████████████| 2.13k/2.13k [07:15<00:00, 4.90it/s, loss=2.76, test loss=2.86]\n",
            "Epoch : 16/19: 100%|████████████████| 2.13k/2.13k [07:16<00:00, 4.89it/s, loss=2.73, test loss=2.85]\n",
            "Epoch : 17/19: 100%|████████████████| 2.13k/2.13k [07:15<00:00, 4.89it/s, loss=2.69, test loss=2.81]\n",
            "Epoch : 18/19: 100%|████████████████| 2.13k/2.13k [07:15<00:00, 4.90it/s, loss=2.66, test loss=2.79]\n",
            "Epoch : 19/19: 100%|████████████████| 2.13k/2.13k [07:13<00:00, 4.92it/s, loss=2.63, test loss=2.77]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pf0rN4RPToom"
      },
      "source": [
        "## Testing / Task 6:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhXbQjP_YrgY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d139c924-6498-4180-e320-c08e5df27c68"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "is_prod = True # production mode or not\n",
        "\n",
        "if is_prod:\n",
        "    model = seq2seqModel.load(path_to_save_models + 'pretrained_moodle.pt')\n",
        "    \n",
        "    to_test = ['I am a student.',\n",
        "               'I have a red car.',  # inversion captured\n",
        "               'I love playing video games.',\n",
        "                'This river is full of fish.', # plein vs pleine (accord)\n",
        "                'The fridge is full of food.',\n",
        "               'The cat fell asleep on the mat.',\n",
        "               'my brother likes pizza.', # pizza is translated to 'la pizza'\n",
        "               'I did not mean to hurt you', # translation of mean in context\n",
        "               'She is so mean',\n",
        "               'Help me pick out a tie to go with this suit!', # right translation\n",
        "               \"I can't help but smoking weed\", # this one and below: hallucination\n",
        "               'The kids were playing hide and seek',\n",
        "               'The cat fell asleep in front of the fireplace']\n",
        "    \n",
        "    for elt in to_test:\n",
        "        print('= = = = = \\n','%s -> %s' % (elt, model.predict(elt)))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "max source index 5281\n",
            "source vocab size 5278\n",
            "max target index 7459\n",
            "target vocab size 7456\n",
            "= = = = = \n",
            " I am a student. -> je suis étudiant . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
            "= = = = = \n",
            " I have a red car. -> j ai une voiture rouge . . . . . . . . . . . . . . . . . . . . . . . . .\n",
            "= = = = = \n",
            " I love playing video games. -> j adore jouer à jeux jeux jeux vidéo . . . . . . . . . . . . . . . . . . . . . .\n",
            "= = = = = \n",
            " This river is full of fish. -> cette rivière est pleine de poisson . . . . . . . . . . . . . . . . . . . . . . . .\n",
            "= = = = = \n",
            " The fridge is full of food. -> le frigo est plein de nourriture . . . . . . . . . . . . . . . . . . . . . . . .\n",
            "= = = = = \n",
            " The cat fell asleep on the mat. -> le chat s est endormi sur le tapis . . . . . . . . . . . . . . . . . . . . . .\n",
            "= = = = = \n",
            " my brother likes pizza. -> mon frère aime la pizza . . . . . . . . . . . . . . . . . . . . . . . . .\n",
            "= = = = = \n",
            " I did not mean to hurt you -> je n ai pas voulu intention de blesser blesser blesser blesser blesser blesser . blesser . blesser . . . . . . . . . . . . .\n",
            "= = = = = \n",
            " She is so mean -> elle est tellement méchant méchant . <EOS>\n",
            "= = = = = \n",
            " Help me pick out a tie to go with this suit! -> aidez moi à chercher une cravate pour aller avec ceci ! ! ! ! ! ! ! ! ! ! ! ! ! ! <EOS>\n",
            "= = = = = \n",
            " I can't help but smoking weed -> je ne peux pas empêcher de de fumer fumer fumer fumer fumer fumer fumer fumer fumer fumer urgence urgence urgence urgence urgence urgence . urgence urgence . urgence urgence .\n",
            "= = = = = \n",
            " The kids were playing hide and seek -> les enfants jouent cache cache cache cache caché caché caché caché caché caché caché caché caché caché caché caché caché caché caché dentifrice perdre caché risques rapide caché risques éveillés\n",
            "= = = = = \n",
            " The cat fell asleep in front of the fireplace -> le chat s est en du du pression peigne peigne cheminée portail portail portail portail portail portail portail portail indépendant oiseaux oiseaux oiseaux oiseaux oiseaux oiseaux oiseaux oiseaux oiseaux oiseaux\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bv9c6-i2kR8"
      },
      "source": [
        "# Question 3 : \n",
        "Write some code to visualize source/target alignments in the style of Fig. 3 in [1] or Fig. 7 in [4] . Provide and interpret your figures for some relevant examples (e.g. to illustrate adjective-noun inversion)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIf07-HA3Bws",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12b0184d-0e22-4744-acb8-0c63c5557c98"
      },
      "source": [
        "from nltk import word_tokenize\n",
        "from itertools import chain\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "##### Write some code to visualize source/target alignments in the style of Fig. 3  #######\n",
        "\n",
        "'''\n",
        "To visualize source/target alignments in the style of Fig. 3, we want the abscisses of the plot to correspond to the words in the source sentence (original version) and the ordinates to correspond to the words\n",
        "in the target (generated) translated sentence. \n",
        "Each pixel shows the weight αij of the annotation of the j-th source word for the i-th target word.\n",
        "We will therefore plot the weights of the generated sentence (target) given an input vector of words (source)\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "def vis_traduction(x,model):\n",
        "  #weight the input sentence and align with targeted one\n",
        "  input_weighted = model.get_attention_weights(x)\n",
        "  input_weighted = [input_weighted[i].tolist() for i in range(len(input_weighted))]\n",
        "\n",
        "  vis = [list(chain.from_iterable(input_weighted[i])) for i in range(len(input_weighted))]\n",
        "  abs = x.split(' ')\n",
        "  ord = model.predict(x).split(' ')\n",
        "\n",
        "  translation_plot = plt.figure(figsize = (15,7))\n",
        "  plt.imshow(vis)\n",
        "  plt.xticks(np.arange(len(vis[0])), abs)\n",
        "  plt.yticks(np.arange(len(vis[1])), ord)\n",
        "\n",
        "  return translation_plot\n",
        "\n",
        "\n",
        "##### Provide and interpret your figures for some relevant examples  #######\n",
        "# Here we will use sentences from the article \"NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE\", in order to compare the results with theirs\n",
        "\n",
        "source_sentences = [\"An admitting privilege is the right of a doctor to admit a patient to a hospital\",\n",
        "                    \"This kind of experience is part of Disney’s efforts\",\n",
        "                    \"Neural machine translation has already shown promising results\"]\n",
        "\n",
        "\n",
        "is_prod = True # production mode or not\n",
        "\n",
        "if is_prod:\n",
        "    model = seq2seqModel.load(path_to_save_models + 'pretrained_moodle.pt')\n",
        "    \n",
        "    to_test = source_sentences\n",
        "    \n",
        "    for x in to_test:\n",
        "        print('= = = = = \\n','%s -> %s' % (x, model.predict(x)))\n",
        "        vis_traduction(x,model)\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "max source index 5281\n",
            "source vocab size 5278\n",
            "max target index 7459\n",
            "target vocab size 7456\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEWDyzE2BlDp"
      },
      "source": [
        "translation_plot(source_sentence[1],rnn_align)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNrr7dWhBlsY"
      },
      "source": [
        "translation_plot(source_sentence[2],rnn_align)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}